{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Setup Environment and Install Packages\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Create directories\n",
        "os.makedirs('medical_rag_bot', exist_ok=True)\n",
        "os.makedirs('medical_rag_bot/docs', exist_ok=True)\n",
        "os.chdir('/content/medical_rag_bot')\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q google-generativeai langchain langchain-google-genai chromadb PyMuPDF langchain-community sentence-transformers faiss-cpu\n",
        "\n",
        "print(\"âœ… Medical RAG Bot Environment Setup Complete!\")\n",
        "print(\"ğŸ“ Directories created: medical_rag_bot/, medical_rag_bot/docs/\")\n",
        "print(\"ğŸ“¦ All packages installed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfTRJ-oWVp8f",
        "outputId": "22c7161e-3db7-48dc-af98-618a9fef0c66"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "âœ… Medical RAG Bot Environment Setup Complete!\n",
            "ğŸ“ Directories created: medical_rag_bot/, medical_rag_bot/docs/\n",
            "ğŸ“¦ All packages installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2: API Configuration & Imports\n",
        "# =============================================================================\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "import time\n",
        "import asyncio\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Get API key from Colab secrets\n",
        "try:\n",
        "    API_KEY = userdata.get('MY_GEMINI_KEY')\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = API_KEY\n",
        "    genai.configure(api_key=API_KEY)\n",
        "    print(\"âœ… Medical Bot API configured successfully!\")\n",
        "    print(\"ğŸ” Gemini API key loaded from secrets\")\n",
        "except Exception as e:\n",
        "    print(\"âŒ Error configuring API key. Please check your GEMINI_API_KEY in Colab secrets\")\n",
        "    print(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UwlUzXmVuPt",
        "outputId": "64af6ef7-e9f9-480f-ab19-385309667207"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Medical Bot API configured successfully!\n",
            "ğŸ” Gemini API key loaded from secrets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3: Document Upload\n",
        "# =============================================================================\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "print(\"ğŸ“‹ Medical Document Upload\")\n",
        "print(\"ğŸ’¡ Please upload your medical PDF or any medical documentation\")\n",
        "print(\"âš¡ Supported formats: PDF files only\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    for filename in uploaded.keys():\n",
        "        if filename.endswith('.pdf'):\n",
        "            shutil.move(filename, f'docs/{filename}')\n",
        "            print(f\"âœ… Medical document uploaded: {filename}\")\n",
        "        else:\n",
        "            print(f\"âš ï¸  Skipping non-PDF file: {filename}\")\n",
        "\n",
        "    pdf_count = len([f for f in os.listdir('docs') if f.endswith('.pdf')])\n",
        "    print(f\"\\nğŸ¥ {pdf_count} medical document(s) ready for processing\")\n",
        "else:\n",
        "    print(\"â„¹ï¸  No files uploaded. You can run this cell again to upload documents.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "sws-pwH7Vxqn",
        "outputId": "b0d7240e-56c4-42ec-a054-47f7223916c8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“‹ Medical Document Upload\n",
            "ğŸ’¡ Please upload your medical PDF or any medical documentation\n",
            "âš¡ Supported formats: PDF files only\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5bd2a1d8-611a-4d33-8ca4-a0e6cd196ba1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5bd2a1d8-611a-4d33-8ca4-a0e6cd196ba1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Medical_book.pdf to Medical_book.pdf\n",
            "âœ… Medical document uploaded: Medical_book.pdf\n",
            "\n",
            "ğŸ¥ 1 medical document(s) ready for processing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4: Document Processing\n",
        "# =============================================================================\n",
        "print(\"ğŸ“– Processing medical documents...\")\n",
        "print(\"â³ This may take a few minutes for large documents (637+ pages)\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "documents = []\n",
        "start_time = time.time()\n",
        "\n",
        "# Load all PDF documents\n",
        "for filename in os.listdir(\"docs\"):\n",
        "    if filename.endswith(\".pdf\"):\n",
        "        print(f\"ğŸ“„ Loading: {filename}\")\n",
        "        loader = PyMuPDFLoader(f\"docs/{filename}\")\n",
        "        docs = loader.load()\n",
        "        documents.extend(docs)\n",
        "        print(f\"   â¤ {len(docs)} pages loaded\")\n",
        "\n",
        "if not documents:\n",
        "    print(\"âŒ No documents found. Please upload PDF files first.\")\n",
        "else:\n",
        "    # Split documents into chunks optimized for medical content\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1500,  # Larger chunks for medical context\n",
        "        chunk_overlap=300,  # More overlap for better continuity\n",
        "        separators=[\"\\n\\n\", \"\\n\", \". \", \", \", \" \", \"\"]\n",
        "    )\n",
        "\n",
        "    print(f\"\\nğŸ”§ Splitting documents into chunks...\")\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "    processing_time = time.time() - start_time\n",
        "    print(f\"\\nâœ… Document processing complete!\")\n",
        "    print(f\"ğŸ“Š Statistics:\")\n",
        "    print(f\"   â€¢ Total pages: {len(documents)}\")\n",
        "    print(f\"   â€¢ Text chunks: {len(chunks)}\")\n",
        "    print(f\"   â€¢ Processing time: {processing_time:.1f} seconds\")\n",
        "    print(f\"   â€¢ Average chunk size: ~{1500} characters\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0y1ZjvgV1wD",
        "outputId": "46fac00d-47ca-495e-e662-289e586c8f52"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“– Processing medical documents...\n",
            "â³ This may take a few minutes for large documents (637+ pages)\n",
            "--------------------------------------------------\n",
            "ğŸ“„ Loading: Medical_book.pdf\n",
            "   â¤ 637 pages loaded\n",
            "\n",
            "ğŸ”§ Splitting documents into chunks...\n",
            "\n",
            "âœ… Document processing complete!\n",
            "ğŸ“Š Statistics:\n",
            "   â€¢ Total pages: 637\n",
            "   â€¢ Text chunks: 2335\n",
            "   â€¢ Processing time: 7.8 seconds\n",
            "   â€¢ Average chunk size: ~1500 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5: Vector Database Creation\n",
        "# =============================================================================\n",
        "print(\"ğŸ§¬ Creating medical knowledge vector database...\")\n",
        "print(\"â³ Generating embeddings for medical content...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    # Initialize embeddings model\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "    # Create vector database with persistence\n",
        "    vectordb = Chroma.from_documents(\n",
        "        chunks,\n",
        "        embeddings,\n",
        "        persist_directory='medical_db',\n",
        "        collection_name=\"medical_knowledge\"\n",
        "    )\n",
        "\n",
        "    # Set up retriever with medical-optimized settings\n",
        "    retriever = vectordb.as_retriever(\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={\"k\": 5}\n",
        "    )\n",
        "\n",
        "    # Initialize LLM with medical-appropriate settings\n",
        "    llm = ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-1.5-flash\",\n",
        "        temperature=0.1,  # Lower temperature for medical accuracy\n",
        "        max_tokens=1024\n",
        "    )\n",
        "\n",
        "    embedding_time = time.time() - start_time\n",
        "\n",
        "    print(f\"âœ… Medical vector database created successfully!\")\n",
        "    print(f\"ğŸ“Š Database info:\")\n",
        "    print(f\"   â€¢ Embeddings model: Google embedding-001\")\n",
        "    print(f\"   â€¢ LLM model: Gemini 1.5 Flash\")\n",
        "    print(f\"   â€¢ Database location: medical_db/\")\n",
        "    print(f\"   â€¢ Processing time: {embedding_time:.1f} seconds\")\n",
        "    print(f\"   â€¢ Retrieval strategy: Top-5 similarity search\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error creating vector database: {e}\")\n",
        "    print(\"Please check your API key and internet connection.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxG8PM1eWKuo",
        "outputId": "e96857e9-9f95-4a7b-89e8-fb235952ba4a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§¬ Creating medical knowledge vector database...\n",
            "â³ Generating embeddings for medical content...\n",
            "--------------------------------------------------\n",
            "âœ… Medical vector database created successfully!\n",
            "ğŸ“Š Database info:\n",
            "   â€¢ Embeddings model: Google embedding-001\n",
            "   â€¢ LLM model: Gemini 1.5 Flash\n",
            "   â€¢ Database location: medical_db/\n",
            "   â€¢ Processing time: 35.0 seconds\n",
            "   â€¢ Retrieval strategy: Top-5 similarity search\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6: Clean Improved Search Function\n",
        "# =============================================================================\n",
        "print(\"ğŸ”§ Loading improved search with clean output...\")\n",
        "\n",
        "def improved_search_with_rag(query):\n",
        "    \"\"\"\n",
        "    Improved search with better query variations and semantic understanding\n",
        "    \"\"\"\n",
        "    # IMPROVED search variations\n",
        "    base_query = query.lower()\n",
        "    search_variations = []\n",
        "\n",
        "    # Original query\n",
        "    search_variations.append(query)\n",
        "\n",
        "    # Clean query (remove question words)\n",
        "    clean_query = query\n",
        "    for remove_word in [\"what are\", \"what is\", \"tell me about\", \"explain\", \"describe\", \"how is\", \"how are\"]:\n",
        "        clean_query = clean_query.replace(remove_word, \"\").strip()\n",
        "    if clean_query != query:\n",
        "        search_variations.append(clean_query)\n",
        "\n",
        "    # Keyword extraction (last 2-3 meaningful words)\n",
        "    words = query.split()\n",
        "    if len(words) >= 2:\n",
        "        search_variations.append(\" \".join(words[-2:]))  # Last 2 words\n",
        "    if len(words) >= 3:\n",
        "        search_variations.append(\" \".join(words[-3:]))  # Last 3 words\n",
        "\n",
        "    # Semantic expansion based on question type\n",
        "    if \"symptom\" in query.lower():\n",
        "        condition_words = [w for w in words if w.lower() not in [\"what\", \"are\", \"is\", \"the\", \"of\", \"symptoms\", \"symptom\"]]\n",
        "        if condition_words:\n",
        "            condition = \" \".join(condition_words)\n",
        "            search_variations.extend([\n",
        "                f\"{condition} symptoms\",\n",
        "                f\"symptoms {condition}\",\n",
        "                f\"{condition} signs\",\n",
        "                f\"{condition} manifestations\",\n",
        "                f\"{condition} effects\",\n",
        "                f\"{condition} complications\"\n",
        "            ])\n",
        "    elif \"treat\" in query.lower():\n",
        "        condition_words = [w for w in words if w.lower() not in [\"what\", \"is\", \"how\", \"treat\", \"treatment\", \"treated\"]]\n",
        "        if condition_words:\n",
        "            condition = \" \".join(condition_words)\n",
        "            search_variations.extend([\n",
        "                f\"{condition} treatment\",\n",
        "                f\"{condition} therapy\",\n",
        "                f\"{condition} management\",\n",
        "                f\"treating {condition}\"\n",
        "            ])\n",
        "    elif \"cause\" in query.lower():\n",
        "        condition_words = [w for w in words if w.lower() not in [\"what\", \"causes\", \"cause\", \"caused\"]]\n",
        "        if condition_words:\n",
        "            condition = \" \".join(condition_words)\n",
        "            search_variations.extend([\n",
        "                f\"{condition} causes\",\n",
        "                f\"{condition} etiology\",\n",
        "                f\"{condition} risk factors\"\n",
        "            ])\n",
        "\n",
        "    # Remove duplicates while preserving order\n",
        "    unique_variations = []\n",
        "    for var in search_variations:\n",
        "        if var.strip() and var not in unique_variations:\n",
        "            unique_variations.append(var)\n",
        "\n",
        "    # Search with all variations and collect results\n",
        "    all_results = []\n",
        "    seen_content = set()\n",
        "\n",
        "    for search_term in unique_variations:\n",
        "        docs = vectordb.similarity_search(search_term, k=8)  # Get more results per variation\n",
        "\n",
        "        for doc in docs:\n",
        "            content_hash = hash(doc.page_content[:100])\n",
        "            if content_hash not in seen_content:\n",
        "                seen_content.add(content_hash)\n",
        "                all_results.append(doc)\n",
        "\n",
        "    # Take top 10 unique results for better coverage\n",
        "    final_chunks = all_results[:10]\n",
        "\n",
        "    if not final_chunks:\n",
        "        return \"âŒ I don't have enough information in the medical encyclopedia to answer this question accurately. Please consult a healthcare professional.\", []\n",
        "\n",
        "    # Create context\n",
        "    context_text = \"\\n\\n\".join([\n",
        "        f\"[ENCYCLOPEDIA SECTION {i+1}]:\\n{doc.page_content}\"\n",
        "        for i, doc in enumerate(final_chunks)\n",
        "    ])\n",
        "\n",
        "    # ENHANCED RAG prompt with semantic understanding\n",
        "    rag_prompt = f\"\"\"You are analyzing content from the Gale Encyclopedia of Medicine to answer a medical question.\n",
        "\n",
        "ENCYCLOPEDIA CONTENT:\n",
        "{context_text}\n",
        "\n",
        "QUESTION: {query}\n",
        "\n",
        "CRITICAL INSTRUCTIONS:\n",
        "1. Focus on SEMANTIC RELEVANCE - look for information that answers the MEANING of the question, not just exact phrase matches\n",
        "2. If asked about \"symptoms,\" look for any descriptions of: signs, symptoms, manifestations, effects, complications, clinical presentations, or how the condition affects patients\n",
        "3. If asked about \"treatment,\" look for: therapy, management, medication, procedures, interventions, or therapeutic approaches\n",
        "4. If asked about \"causes,\" look for: etiology, risk factors, pathophysiology, or contributing factors\n",
        "5. Extract ALL relevant information that addresses the question's intent, even if worded differently\n",
        "6. Look across ALL encyclopedia sections for complementary information\n",
        "7. Synthesize information from multiple sections when they relate to the same topic\n",
        "8. Quote exact phrases when describing specific medical findings or lists\n",
        "9. If you find semantically relevant information, provide a comprehensive answer\n",
        "10. If the encyclopedia content does NOT contain information that meaningfully addresses the question, respond with: \"INSUFFICIENT_ENCYCLOPEDIA_INFO\"\n",
        "11. Include appropriate medical disclaimers\n",
        "\n",
        "SEMANTIC MATCHING EXAMPLES:\n",
        "- \"What are diabetes symptoms?\" â†’ Look for descriptions of how diabetes affects patients, clinical signs, manifestations\n",
        "- \"How is cancer treated?\" â†’ Look for therapy approaches, medications, procedures, management strategies\n",
        "- \"What causes heart disease?\" â†’ Look for risk factors, etiology, pathophysiology, contributing conditions\n",
        "\n",
        "Pay attention to the MEANING and INTENT of the question, not just keyword matching.\n",
        "\n",
        "ENCYCLOPEDIA-BASED ANSWER:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = llm.invoke(rag_prompt)\n",
        "        content = response.content if hasattr(response, \"content\") else str(response)\n",
        "\n",
        "        if \"INSUFFICIENT_ENCYCLOPEDIA_INFO\" in content.upper():\n",
        "            return \"âŒ I don't have enough information in the medical encyclopedia to answer this question accurately. Please consult a healthcare professional.\", final_chunks\n",
        "\n",
        "        return content, final_chunks\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Error analyzing encyclopedia content: {e}\", final_chunks\n",
        "\n",
        "print(\"âœ… Clean search function loaded!\")\n",
        "print(\"ğŸ¯ Semantic understanding with minimal output noise\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVGM9fVXjFEG",
        "outputId": "2f1c63c6-b5ab-424b-ef44-141dd608310c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Loading improved search with clean output...\n",
            "âœ… Clean search function loaded!\n",
            "ğŸ¯ Semantic understanding with minimal output noise\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7: Clean LLM Fallback System\n",
        "# =============================================================================\n",
        "print(\"ğŸ”§ Setting up LLM Fallback System...\")\n",
        "\n",
        "def ask_llm_only(query):\n",
        "    \"\"\"\n",
        "    Try LLM first - if uncertain, return None to trigger RAG search\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"You are a medical AI assistant. Answer the following medical question ONLY if you are completely confident and certain about the answer.\n",
        "\n",
        "STRICT INSTRUCTIONS:\n",
        "- If you are even slightly uncertain, unsure, or lack complete confidence, respond with exactly: \"MEDICAL_UNCERTAIN\"\n",
        "- Only provide answers for well-established medical knowledge you are absolutely certain about\n",
        "- Do not guess, infer, or provide partial answers\n",
        "- Do not provide specific medical advice, diagnoses, or treatment recommendations\n",
        "- Always include appropriate medical disclaimers when you do answer\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer only if completely certain, otherwise respond \"MEDICAL_UNCERTAIN\":\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = llm.invoke(prompt)\n",
        "        content = response.content if hasattr(response, \"content\") else str(response)\n",
        "\n",
        "        # Check if LLM indicated uncertainty\n",
        "        if any(phrase in content.upper() for phrase in [\"MEDICAL_UNCERTAIN\", \"UNCERTAIN\", \"UNSURE\", \"DON'T KNOW\", \"NOT SURE\", \"I DON'T\", \"CANNOT BE CERTAIN\"]):\n",
        "            return None\n",
        "\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ LLM Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def ask_with_rag(query):\n",
        "    \"\"\"\n",
        "    Use RAG when LLM is uncertain - search encyclopedia and analyze\n",
        "    \"\"\"\n",
        "    print(\"   ğŸ” Searching encyclopedia...\")\n",
        "\n",
        "    # Multiple search strategies with more results\n",
        "    search_variations = [\n",
        "        query,\n",
        "        query.replace(\"what are\", \"\").replace(\"what is\", \"\").replace(\"?\", \"\").strip(),\n",
        "        f\"treatment {query.split()[-1]}\" if any(word in query.upper() for word in [\"AIDS\", \"HIV\", \"CANCER\", \"DIABETES\"]) else query,\n",
        "        \" \".join(query.split()[-3:])  # Last 3 words\n",
        "    ]\n",
        "\n",
        "    all_chunks = []\n",
        "    seen_content = set()\n",
        "\n",
        "    for search_term in search_variations:\n",
        "        docs = vectordb.similarity_search(search_term, k=10)\n",
        "\n",
        "        for doc in docs:\n",
        "            content_hash = hash(doc.page_content[:100])\n",
        "            if content_hash not in seen_content:\n",
        "                seen_content.add(content_hash)\n",
        "                all_chunks.append(doc)\n",
        "\n",
        "    # Keep top 6 chunks\n",
        "    final_chunks = all_chunks[:6]\n",
        "\n",
        "    if not final_chunks:\n",
        "        return \"âŒ I don't have enough information in the medical encyclopedia to answer this question accurately. Please consult a healthcare professional.\", []\n",
        "\n",
        "    print(f\"   ğŸ“„ Analyzing {len(final_chunks)} sections...\")\n",
        "\n",
        "    # Create context from retrieved chunks\n",
        "    context_text = \"\\n\\n\".join([\n",
        "        f\"[ENCYCLOPEDIA SECTION {i+1}]:\\n{doc.page_content}\"\n",
        "        for i, doc in enumerate(final_chunks)\n",
        "    ])\n",
        "\n",
        "    # RAG prompt focused on encyclopedia content\n",
        "    rag_prompt = f\"\"\"You are analyzing content from the Gale Encyclopedia of Medicine to answer a medical question.\n",
        "\n",
        "ENCYCLOPEDIA CONTENT:\n",
        "{context_text}\n",
        "\n",
        "QUESTION: {query}\n",
        "\n",
        "CRITICAL INSTRUCTIONS:\n",
        "1. Focus on SEMANTIC RELEVANCE - look for information that answers the MEANING of the question, not just exact phrase matches\n",
        "2. If asked about \"symptoms,\" look for any descriptions of: signs, symptoms, manifestations, effects, complications, clinical presentations, or how the condition affects patients\n",
        "3. If asked about \"treatment,\" look for: therapy, management, medication, procedures, interventions, or therapeutic approaches\n",
        "4. If asked about \"causes,\" look for: etiology, risk factors, pathophysiology, or contributing factors\n",
        "5. Extract ALL relevant information that addresses the question's intent, even if worded differently\n",
        "6. Look across ALL encyclopedia sections for complementary information\n",
        "7. Synthesize information from multiple sections when they relate to the same topic\n",
        "8. Quote exact phrases when describing specific medical findings or lists\n",
        "9. If you find semantically relevant information, provide a comprehensive answer\n",
        "10. If the encyclopedia content does NOT contain information that meaningfully addresses the question, respond with: \"INSUFFICIENT_ENCYCLOPEDIA_INFO\"\n",
        "11. Include appropriate medical disclaimers\n",
        "\n",
        "ENCYCLOPEDIA-BASED ANSWER:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = llm.invoke(rag_prompt)\n",
        "        content = response.content if hasattr(response, \"content\") else str(response)\n",
        "\n",
        "        # Check if encyclopedia had insufficient info\n",
        "        if \"INSUFFICIENT_ENCYCLOPEDIA_INFO\" in content.upper():\n",
        "            return \"âŒ I don't have enough information in the medical encyclopedia to answer this question accurately. Please consult a healthcare professional.\", final_chunks\n",
        "\n",
        "        return content, final_chunks\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Error analyzing encyclopedia content: {e}\", final_chunks\n",
        "\n",
        "def medical_fallback_response(query):\n",
        "    \"\"\"\n",
        "    Clean version: Try LLM first, fallback to RAG if uncertain, admit ignorance if neither works\n",
        "    \"\"\"\n",
        "    # Step 1: Try LLM direct knowledge first\n",
        "    llm_response = ask_llm_only(query)\n",
        "\n",
        "    if llm_response:\n",
        "        # LLM was confident\n",
        "        return f\"[ğŸ§  DIRECT] {llm_response}\", \"direct\", [], False\n",
        "\n",
        "    # Step 2: LLM was uncertain, try RAG\n",
        "    rag_response, chunks = improved_search_with_rag(query)\n",
        "\n",
        "    if rag_response and not rag_response.startswith(\"âŒ\"):\n",
        "        # RAG found good information\n",
        "        return f\"[ğŸ“š RAG] {rag_response}\", \"rag\", chunks, True\n",
        "\n",
        "    # Step 3: Neither LLM nor RAG could answer confidently\n",
        "    return \"\"\"[âŒ UNKNOWN] I don't have enough reliable information to answer this question accurately.\n",
        "\n",
        "This could be because:\n",
        "â€¢ The question is outside my knowledge base\n",
        "â€¢ The medical encyclopedia doesn't contain this specific information\n",
        "â€¢ The topic requires specialized expertise\n",
        "\n",
        "ğŸ¥ RECOMMENDATION: Please consult a qualified healthcare professional who can provide accurate, personalized medical information for your specific situation.\n",
        "\n",
        "âš ï¸ It's always better to admit uncertainty than to provide potentially incorrect medical information.\"\"\", \"unknown\", [], False\n",
        "\n",
        "def show_encyclopedia_sources(chunks, show_chunks=False):\n",
        "    \"\"\"\n",
        "    Display encyclopedia sources cleanly\n",
        "    \"\"\"\n",
        "    if not chunks:\n",
        "        return\n",
        "\n",
        "    if show_chunks:\n",
        "        print(f\"\\nğŸ“‹ Sources:\")\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            page_info = f\"Page {chunk.metadata.get('page', 'Unknown')}\"\n",
        "            preview = chunk.page_content[:200] + \"...\"\n",
        "            print(f\"â€¢ {page_info}: {preview}\")\n",
        "\n",
        "print(\"âœ… Clean LLM Fallback System loaded!\")\n",
        "print(\"ğŸ¨ Simplified processing messages\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iblPGk8-XA1_",
        "outputId": "630fb1f8-5701-466c-b987-3c254cc9a54c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Setting up LLM Fallback System...\n",
            "âœ… Clean LLM Fallback System loaded!\n",
            "ğŸ¨ Simplified processing messages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 8: Simple Medical Chat Interface (Replace your current Cell 8 with this)\n",
        "# =============================================================================\n",
        "\n",
        "class MedicalChat:\n",
        "    def __init__(self):\n",
        "        self.show_sources = False\n",
        "        self.running = True\n",
        "\n",
        "    def start_chat(self):\n",
        "        \"\"\"Start the interactive medical chat\"\"\"\n",
        "        self.print_header()\n",
        "        self.chat_loop()\n",
        "\n",
        "    def print_header(self):\n",
        "        \"\"\"Print the chatbot header\"\"\"\n",
        "        print(\"ğŸ¥ MEDICAL LLM FALLBACK CHATBOT\")\n",
        "        print(\"=\"*60)\n",
        "        print(\"ğŸ”„ Smart Response System:\")\n",
        "        print(\"   â€¢ ğŸ§  DIRECT: General medical knowledge\")\n",
        "        print(\"   â€¢ ğŸ“š RAG: Encyclopedia-based research\")\n",
        "        print(\"   â€¢ âŒ UNKNOWN: Admits uncertainty\")\n",
        "        print(\"\\nğŸ›¡ï¸ Safety: Prevents misinformation â€¢ Admits limitations\")\n",
        "        print(\"âš ï¸  Educational purposes only - consult healthcare professionals\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        if not documents:\n",
        "            print(\"âš ï¸  No medical documents loaded!\")\n",
        "            return False\n",
        "        else:\n",
        "            print(f\"ğŸ“š Medical Encyclopedia: {len(documents)} pages loaded\")\n",
        "\n",
        "        print(\"\\nğŸ’¡ Example questions to try:\")\n",
        "        print(\"   ğŸ§  'What is diabetes?' (likely DIRECT)\")\n",
        "        print(\"   ğŸ“š 'What are the four considerations for AIDS treatment?' (likely RAG)\")\n",
        "        print(\"   âŒ 'What COVID variant was discovered yesterday?' (likely UNKNOWN)\")\n",
        "        print(\"\\nğŸ’¡ Commands: 'sources on/off', 'exit'\")\n",
        "        return True\n",
        "\n",
        "    def chat_loop(self):\n",
        "        \"\"\"Main chat interaction loop\"\"\"\n",
        "        while self.running:\n",
        "            try:\n",
        "                user_input = input(\"\\nğŸ’¬ Your question: \").strip()\n",
        "\n",
        "                if not user_input:\n",
        "                    continue\n",
        "\n",
        "                self.handle_input(user_input)\n",
        "\n",
        "            except KeyboardInterrupt:\n",
        "                print(\"\\n\\nğŸ‘‹ Goodbye!\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"\\nâŒ Error: {e}\")\n",
        "\n",
        "    def handle_input(self, user_input):\n",
        "        \"\"\"Process user input and commands\"\"\"\n",
        "        command = user_input.lower()\n",
        "\n",
        "        # Handle commands\n",
        "        if command in ['exit', 'quit', 'stop']:\n",
        "            print(\"\\nğŸ‘‹ Thank you for using the medical chatbot!\")\n",
        "            self.running = False\n",
        "            return\n",
        "\n",
        "        elif command == 'sources on':\n",
        "            self.show_sources = True\n",
        "            print(\"âœ… Detailed sources enabled\")\n",
        "            return\n",
        "\n",
        "        elif command == 'sources off':\n",
        "            self.show_sources = False\n",
        "            print(\"âœ… Detailed sources disabled\")\n",
        "            return\n",
        "\n",
        "        # Process medical question\n",
        "        self.process_question(user_input)\n",
        "\n",
        "    def process_question(self, question):\n",
        "        \"\"\"Process a medical question\"\"\"\n",
        "        print(f\"\\nğŸ’¬ {question}\")\n",
        "\n",
        "        # Get response using your existing function\n",
        "        response, mode, chunks, has_sources = medical_fallback_response(question)\n",
        "\n",
        "        # Display response\n",
        "        self.display_response(response, chunks, has_sources)\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    def display_response(self, response, chunks, has_sources):\n",
        "        \"\"\"Display the response based on type\"\"\"\n",
        "        if \"[ğŸ§  DIRECT]\" in response:\n",
        "            clean_text = response.replace(\"[ğŸ§  DIRECT]\", \"\").strip()\n",
        "            print(f\"\\nğŸ§  DIRECT: {clean_text}\")\n",
        "\n",
        "        elif \"[ğŸ“š RAG]\" in response:\n",
        "            clean_text = response.replace(\"[ğŸ“š RAG]\", \"\").strip()\n",
        "            print(f\"\\nğŸ“š RAG: {clean_text}\")\n",
        "            self.show_source_info(chunks, has_sources)\n",
        "\n",
        "        elif \"[âŒ UNKNOWN]\" in response:\n",
        "            clean_text = response.replace(\"[âŒ UNKNOWN]\", \"\").strip()\n",
        "            print(f\"\\nâŒ UNKNOWN: {clean_text}\")\n",
        "\n",
        "    def show_source_info(self, chunks, has_sources):\n",
        "        \"\"\"Show source information if available\"\"\"\n",
        "        if has_sources and chunks:\n",
        "            if self.show_sources:\n",
        "                print(f\"\\nğŸ“‹ Detailed Sources:\")\n",
        "                for i, chunk in enumerate(chunks[:3]):\n",
        "                    page_num = chunk.metadata.get('page', 'Unknown')\n",
        "                    preview = chunk.page_content[:200] + \"...\"\n",
        "                    print(f\"â€¢ Page {page_num}: {preview}\")\n",
        "            else:\n",
        "                page_list = [str(chunk.metadata.get('page', '?')) for chunk in chunks[:5]]\n",
        "                print(f\"\\nğŸ“‹ Sources: Encyclopedia pages {', '.join(page_list)}\")\n",
        "\n",
        "def ask_single_question(question, show_sources=False):\n",
        "    \"\"\"Ask a single question without starting interactive chat\"\"\"\n",
        "    if not question.strip():\n",
        "        print(\"âŒ Please provide a question.\")\n",
        "        return\n",
        "\n",
        "    print(f\"ğŸ’¬ {question}\")\n",
        "\n",
        "    # Get response\n",
        "    response, mode, chunks, has_sources = medical_fallback_response(question)\n",
        "\n",
        "    # Create temporary chat instance to use display method\n",
        "    chat = MedicalChat()\n",
        "    chat.show_sources = show_sources\n",
        "    chat.display_response(response, chunks, has_sources)\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Create the chat instance\n",
        "medical_chat = MedicalChat()\n",
        "\n",
        "# Show header immediately\n",
        "medical_chat.print_header()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ğŸš€ HOW TO USE:\")\n",
        "print(\"=\"*50)\n",
        "print(\"Interactive Chat:\")\n",
        "print(\"   medical_chat.start_chat()\")\n",
        "print(\"\\nSingle Questions:\")\n",
        "print(\"   ask_single_question('What is diabetes?')\")\n",
        "print(\"   ask_single_question('AIDS treatment', show_sources=True)\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4xKQ3dghwF5",
        "outputId": "a10a032a-5eca-4958-f067-6dcbb3bd07dd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¥ MEDICAL LLM FALLBACK CHATBOT\n",
            "============================================================\n",
            "ğŸ”„ Smart Response System:\n",
            "   â€¢ ğŸ§  DIRECT: General medical knowledge\n",
            "   â€¢ ğŸ“š RAG: Encyclopedia-based research\n",
            "   â€¢ âŒ UNKNOWN: Admits uncertainty\n",
            "\n",
            "ğŸ›¡ï¸ Safety: Prevents misinformation â€¢ Admits limitations\n",
            "âš ï¸  Educational purposes only - consult healthcare professionals\n",
            "------------------------------------------------------------\n",
            "ğŸ“š Medical Encyclopedia: 637 pages loaded\n",
            "\n",
            "ğŸ’¡ Example questions to try:\n",
            "   ğŸ§  'What is diabetes?' (likely DIRECT)\n",
            "   ğŸ“š 'What are the four considerations for AIDS treatment?' (likely RAG)\n",
            "   âŒ 'What COVID variant was discovered yesterday?' (likely UNKNOWN)\n",
            "\n",
            "ğŸ’¡ Commands: 'sources on/off', 'exit'\n",
            "\n",
            "==================================================\n",
            "ğŸš€ HOW TO USE:\n",
            "==================================================\n",
            "Interactive Chat:\n",
            "   medical_chat.start_chat()\n",
            "\n",
            "Single Questions:\n",
            "   ask_single_question('What is diabetes?')\n",
            "   ask_single_question('AIDS treatment', show_sources=True)\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "medical_chat.start_chat()"
      ],
      "metadata": {
        "id": "O9-92hrdlm67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9625e049-1f8d-4699-c4f4-d02be011529b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¥ MEDICAL LLM FALLBACK CHATBOT\n",
            "============================================================\n",
            "ğŸ”„ Smart Response System:\n",
            "   â€¢ ğŸ§  DIRECT: General medical knowledge\n",
            "   â€¢ ğŸ“š RAG: Encyclopedia-based research\n",
            "   â€¢ âŒ UNKNOWN: Admits uncertainty\n",
            "\n",
            "ğŸ›¡ï¸ Safety: Prevents misinformation â€¢ Admits limitations\n",
            "âš ï¸  Educational purposes only - consult healthcare professionals\n",
            "------------------------------------------------------------\n",
            "ğŸ“š Medical Encyclopedia: 637 pages loaded\n",
            "\n",
            "ğŸ’¡ Example questions to try:\n",
            "   ğŸ§  'What is diabetes?' (likely DIRECT)\n",
            "   ğŸ“š 'What are the four considerations for AIDS treatment?' (likely RAG)\n",
            "   âŒ 'What COVID variant was discovered yesterday?' (likely UNKNOWN)\n",
            "\n",
            "ğŸ’¡ Commands: 'sources on/off', 'exit'\n",
            "\n",
            "ğŸ’¬ Your question: what is diabetes\n",
            "\n",
            "ğŸ’¬ what is diabetes\n",
            "\n",
            "ğŸ§  DIRECT: Diabetes is a group of metabolic disorders characterized by hyperglycemia resulting from defects in insulin secretion, insulin action, or both.  This leads to elevated levels of glucose in the blood.\n",
            "\n",
            "**Disclaimer:** This information is for general knowledge and educational purposes only, and does not constitute medical advice.  It is essential to consult with a qualified healthcare professional for any health concerns or before making any decisions related to your health or treatment.\n",
            "--------------------------------------------------\n",
            "\n",
            "ğŸ’¬ Your question: tell me everything about diabetes\n",
            "\n",
            "ğŸ’¬ tell me everything about diabetes\n",
            "\n",
            "ğŸ“š RAG: The provided text from the Gale Encyclopedia of Medicine offers information on diabetes mellitus, specifically Type I diabetes.  This information should not be considered medical advice.  Always consult a healthcare professional for diagnosis and treatment.\n",
            "\n",
            "**Causes:**\n",
            "\n",
            "The encyclopedia states that in diabetes mellitus, \"A person...either does not make enough insulin, or makes insulin that does not work properly. The result is blood sugar that remains high, a condition called hyperglycemia.\"  Additionally, Section 9 mentions that Type I diabetes \"may be caused by an antibody that attacks and destroys the islet cells of the pancreas, which produce insulin.\"\n",
            "\n",
            "**Symptoms and Effects:**\n",
            "\n",
            "High blood sugar (hyperglycemia) is a key characteristic.  Untreated diabetes can cause damage or failure of \"the eyes, kidneys, nerves, heart, blood vessels, and other body organs.\" Section 10 lists fatigue and \"an abnormally high level of glucose in the blood (hyperglycemia)\" as symptoms of Type I diabetes.  The encyclopedia also mentions hypoglycemia (low blood sugar), caused by various hormone disorders and liver disease, as well as by too much insulin.  Brain damage can occur from glucose levels below 40 mg/dL and coma from levels above 470 mg/dL.\n",
            "\n",
            "**Diagnosis:**\n",
            "\n",
            "Diabetes diagnosis requires testing.  The encyclopedia describes several blood sugar tests:\n",
            "\n",
            "*   **Whole blood glucose test:** A finger-stick test using a glucometer.\n",
            "*   **Fasting plasma glucose test:** Blood drawn from a vein after an eight-hour fast.\n",
            "*   **Oral glucose tolerance test:** Measures glucose levels before and two hours after drinking a glucose beverage, also requiring an eight-hour fast.  A consistent diet with at least 150g of carbohydrates daily for three days prior is also required.\n",
            "*   **Gestational oral glucose tolerance test:** A variation used to screen pregnant women at 24-28 weeks.\n",
            "\n",
            "Abnormal results on these tests indicate diabetes and must be confirmed with repeat testing.  Specific values for normal and abnormal results are provided in Sections 3 and 6.\n",
            "\n",
            "**Treatment and Management:**\n",
            "\n",
            "The encyclopedia mentions that the American Diabetes Association (ADA) recommends that people with diabetes \"keep the amount of glucose in the blood at a normal level as much as possible,\" which can be achieved through home blood sugar testing one or more times a day.  Section 8 lists various antidiabetic drugs with their potential side effects, including Diabinese, Glucotrol, Micronase, Orinase, and insulin.  The encyclopedia emphasizes the importance of consulting a doctor for interpretation of test results and management of the condition.  It also notes that individuals may need to stop certain medications before testing.\n",
            "\n",
            "\n",
            "**Additional Notes:**\n",
            "\n",
            "The encyclopedia provides resources including organizations (American Diabetes Association, Centers for Disease Control and Prevention, National Diabetes Information Clearinghouse, National Institute of Diabetes and Digestive and Kidney Diseases) and publications for further information.  The information provided is limited to what is explicitly stated within the provided text excerpts.\n",
            "\n",
            "ğŸ“‹ Sources: Encyclopedia pages 542, 543, 543, 542, 542\n",
            "--------------------------------------------------\n",
            "\n",
            "ğŸ’¬ Your question: What COVID variant was discovered yesterday?\n",
            "\n",
            "ğŸ’¬ What COVID variant was discovered yesterday?\n",
            "\n",
            "âŒ UNKNOWN: I don't have enough reliable information to answer this question accurately.\n",
            "\n",
            "This could be because:\n",
            "â€¢ The question is outside my knowledge base\n",
            "â€¢ The medical encyclopedia doesn't contain this specific information\n",
            "â€¢ The topic requires specialized expertise\n",
            "\n",
            "ğŸ¥ RECOMMENDATION: Please consult a qualified healthcare professional who can provide accurate, personalized medical information for your specific situation.\n",
            "\n",
            "âš ï¸ It's always better to admit uncertainty than to provide potentially incorrect medical information.\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FIEIrXl7WIJK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}